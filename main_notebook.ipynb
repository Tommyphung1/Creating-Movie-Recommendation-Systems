{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project 4 Project\n",
    "## Recommendations of Movies\n",
    "![First Picture](pictures/Movie.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "There are more than millions of movies made since first introduced. The normal person however has limited time to watch movies in their free time more than ever. **To help consumers save time and money and companies give consumers the best experience**, recommendations are made to make selection easier. To make these recommendations, we will be using a dataset of roughly **10,000 movie entries** to predict that a user would rate a given movie higher than those users with similar reviews on said movie. By comparing similar users and movie ratings, we should be able to recommend movies accurately. \n",
    "\n",
    "We attempt to use **memory-based modeling** and **model-based modeling** to fit the training set. **Peterson's similarity** appeared to perform the best for the Memory based models and was used to compare the others. Grid search was also used for both types to find the best combinations for each of the models. The final model used was the KNN Baseline algorithm\n",
    "\n",
    "Lastly, there were **two ways** to recommend to users the top 5 movies. <br>\n",
    "**First** was an artificial ranking given by the position and weighing them the user-based model and the item-based model. **The sum of the two ranks is their combined rank and is compared to the others to recommend.** <br>\n",
    "The **second** way is to take the corresponding rank of the predicted values from the models and take **the average of the ratings from their sum**. The second way appear to be a better prediction than the previous models with a small difference in RSME. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buiness Understanding\n",
    "There has been a boom in streaming services and thousands of movies for consumers to watch. Netflix has over **4,000 movies** and Prime Video has roughly **7,000 movies** not considering that these big streaming services have their original movies as well. To compete with other streaming platforms, the user experience should be the focus of these companies. <br>\n",
    "\n",
    "**One aspect to look into is the recommendation system they have on their website that would recommend movies to the users based on their movies, trending movies, and popular movies**. The user experience needs to entice old users to stay and welcome new users to join and begin watching movies. \n",
    "\n",
    "Recommendation works well in most cases. On average, better-rated movies perform well and people will actively look for them before making their choice to watch said movie. For example, **70% of videos** watched are made from their recommendations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset was compiled by the **Grouplens** research group and the source of that data comes from [MovieLens](https://movielens.org/). MovieLens is a movie recommendation service that has **9,742 movies** and **100,836 ratings** from **610 users**. The dataset was updated on September 26, 2018. There are three datasets but will only be needing two of them, the ratings and the movies. \n",
    "\n",
    "The movie dataset has the **movie IDs, titles, and genres** for the movie. This was used mainly for conversions but could potentially be used for the genres as it was explored for a bit. \n",
    "Ratings dataset have the most pieces of information as it has ratings and timestamps. The ratings were scaled from **0 to 5 with a .5 step**. \n",
    "\n",
    "Exploring the datasets have some interesting facts concerning the dataset. For one, the distribution is slightly skewed left with the **mean** rating happening to be roughly **3.5**. There are a handful of users that contributes to the reviews given which may have bias depending on who the user was. **Most movies were not rated below 3** so it might be difficult to determine a good movie to recommend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the dataset and load them in with proper names ###\n",
    "### Links were not used for this project ###\n",
    "\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "movies = pd.read_csv('Data/movies.csv')\n",
    "ratings = pd.read_csv('Data/ratings.csv')\n",
    "tags = pd.read_csv('Data/tags.csv')\n",
    "\n",
    "# links = pd.read_csv('ml-latest-small/links.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration Information\n",
    "* There are **9,742** movies in the dataset\n",
    "* There are **100,836** ratings and 610 users \n",
    "* There are **58** users that make up the **3,683** tags added\n",
    "* Genres are separated by | if there are more than one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9742 entries, 0 to 9741\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  9742 non-null   int64 \n",
      " 1   title    9742 non-null   object\n",
      " 2   genres   9742 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 228.5+ KB\n"
     ]
    }
   ],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperations\n",
    "When exploring the dataset, none of the data was missing and other than the timestamps, all columns were usable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove the timestamp column ### \n",
    "ratings.drop('timestamp', axis = 1, inplace = True)\n",
    "movies['genres'] = movies['genres'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "### Reader to use surprise libraries and create training set and testset ### \n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings,reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.3, random_state= 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary libraries for modeling and validating and testing accuracy of the models ### \n",
    "from surprise.prediction_algorithms import knns\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "We used **surprise library** to create a recommendation system using different algorithms. The surprise library has bulting models and testing function that can be easily used in this project. The Memory-Based models are **KnnBasic**, **KnnBaseline**, and **KnnWithMeans**. The Model-based modeling algorithm is **SVD or Singular Value decomposition**. After finding the best base model, a grid search is used to find the best parameters. We also tried to use different similarity conditions to see if that also improves the model. \n",
    "\n",
    "The metric used for evaluation is **RSME or the Root Square Mean Error**. This metric gives the average amount that each predicted rating was off by. Ideally, we would want a score close to 0 to predict similarly to other rated movies in the test set. \n",
    "\n",
    "The **default paramenters** are used to compare the model. We perform a grid search once the best performing model is choosing. We will be using **cross validation** function to get an average on the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Memory Based Methods \n",
    "**Three different** variation of KNN with all the cosine similarity to compare and all user based for these models. <br>\n",
    "Cosine similarity perform better for KnnBasic and KnnWithMeans, but the best performing model, **KnnBaseline** perform best with **Pearson similarity** instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnnBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.9780677232980256\n",
      "RMSE: 0.9819\n",
      "Testset RSME Score:  0.9818952607403942\n"
     ]
    }
   ],
   "source": [
    "sim_cosine = {\"name\": \"cosine\", \"user_based\": True}\n",
    "basic = knns.KNNBasic(sim_options=sim_cosine, random_state = 69)\n",
    "cv_basic = cross_validate(basic, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "basic.fit(trainset)\n",
    "basic_pred = basic.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_basic)\n",
    "print('Testset RSME Score: ',accuracy.rmse(basic_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnnWithMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.9081005845177618\n",
      "RMSE: 0.9103\n",
      "Testset RSME Score:  0.910278017316939\n"
     ]
    }
   ],
   "source": [
    "knn_means = knns.KNNWithMeans(sim_options=sim_cosine, random_state = 69)\n",
    "cv_means = cross_validate(knn_means, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_means.fit(trainset)\n",
    "predictions = knn_means.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_means)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.8860220078078452\n",
      "RMSE: 0.8883\n",
      "Testset RSME Score:  0.8882986936389531\n"
     ]
    }
   ],
   "source": [
    "knn_baseline = knns.KNNBaseline(sim_options=sim_cosine, random_state = 69)\n",
    "cv_baseline = cross_validate(knn_baseline, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_baseline)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVD algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validate RMSE Score:  0.8740699058628424\n",
      "RMSE: 0.8833\n",
      "Testset RSME Score:  0.883294923683403\n"
     ]
    }
   ],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "svd = SVD(random_state = 69)\n",
    "svd_cv = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)['test_rmse'].mean()\n",
    "svd.fit(trainset)\n",
    "svd_pred = svd.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', svd_cv)\n",
    "print('Testset RSME Score: ',accuracy.rmse(svd_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KnnBaseline vs SVD\n",
    "Both KnnBaseline and SVD performed the best when modeling so we need to determine which will be used for the final model. A gridsearch is used to see if there is a better combination of parameters to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "# params = {'k':[10, 20, 30, 40, 50],\n",
    "#           'min_k': [1, 2, 3, 4,5,6,7,8,9,10],\n",
    "#           'random_state':[69]\n",
    "#          }\n",
    "# g_s_baseline = GridSearchCV(knns.KNNBaseline,param_grid=params,n_jobs=-1)\n",
    "# g_s_baseline.fit(data)\n",
    "# g_s_baseline.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.8728894348485715\n",
      "RMSE: 0.8753\n",
      "Testset RSME Score:  0.8753167080242304\n"
     ]
    }
   ],
   "source": [
    "sim_pearson = {\"name\": \"pearson\", \"user_based\": True}\n",
    "knn_baseline = knns.KNNBaseline(k = 30, min_k = 6, sim_options=sim_pearson, random_state = 69)\n",
    "cv_baseline_best = cross_validate(knn_baseline, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "print('Average Cross Validate RMSE Score: ', cv_baseline_best)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "# params = {'n_factors': [20, 50, 100],\n",
    "#          'reg_all': [0.02, 0.05, 0.1],\n",
    "#           'lr_all': [.001, .002, .003, .004, .005],\n",
    "#          'random_state':[69]\n",
    "#          }\n",
    "# g_s_svd = GridSearchCV(SVD,param_grid=params,n_jobs=-1,cv = 5)\n",
    "# g_s_svd.fit(data)\n",
    "# g_s_svd.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-15ced4a863c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_svd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_factors\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_all\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m69\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcv_svd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_svd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RMSE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MAE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_rmse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbest_svd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msvd_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_svd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVD' is not defined"
     ]
    }
   ],
   "source": [
    "best_svd = SVD(n_factors= 50, reg_all = 0.05, lr_all= 0.005, random_state = 69)\n",
    "cv_svd = cross_validate(best_svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)['test_rmse'].mean()\n",
    "best_svd.fit(trainset)\n",
    "svd_pred = best_svd.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_svd)\n",
    "print('Testset RSME Score: ',accuracy.rmse(svd_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "KnnBaseline perform slightly better than the SVD algorithm so we will using KnnBaseline as our main model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
