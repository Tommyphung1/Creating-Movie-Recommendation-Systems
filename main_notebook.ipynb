{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project 4 Project\n",
    "## Recommendations of Movies\n",
    "![First Picture](pictures/Movie.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "There are more than millions of movies made since first introduced. The normal person however has limited time to watch movies in their free time more than ever. **To help consumers save time and money and companies give consumers the best experience**, recommendations are made to make selection easier. To make these recommendations, we will be using a dataset of roughly **10,000 movie entries** to predict that a user would rate a given movie higher than those users with similar reviews on said movie. By comparing similar users and movie ratings, we should be able to recommend movies accurately. \n",
    "\n",
    "We attempt to use **memory-based modeling** and **model-based modeling** to fit the training set. **Peterson's similarity** appeared to perform the best for the Memory based models and was used to compare the others. Grid search was also used for both types to find the best combinations for each of the models. The final model used was the KNN Baseline algorithm\n",
    "\n",
    "Lastly, there were **two ways** to recommend to users the top 5 movies. <br>\n",
    "**First** was an artificial ranking given by the position and weighing them the user-based model and the item-based model. **The sum of the two ranks is their combined rank and is compared to the others to recommend.** <br>\n",
    "The **second** way is to take the corresponding rank of the predicted values from the models and take **the average of the ratings from their sum**. The second way appear to be a better prediction than the previous models with a small difference in RSME. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buiness Understanding\n",
    "There has been a boom in streaming services and thousands of movies for consumers to watch. Netflix has over **4,000 movies** and Prime Video has roughly **7,000 movies** not considering that these big streaming services have their original movies as well. To compete with other streaming platforms, the user experience should be the focus of these companies. <br>\n",
    "\n",
    "**One aspect to look into is the recommendation system they have on their website that would recommend movies to the users based on their movies, trending movies, and popular movies**. The user experience needs to entice old users to stay and welcome new users to join and begin watching movies. \n",
    "\n",
    "Recommendation works well in most cases. On average, better-rated movies perform well and people will actively look for them before making their choice to watch said movie. For example, **70% of videos** watched are made from their recommendations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset was compiled by the **Grouplens** research group and the source of that data comes from [MovieLens](https://movielens.org/). MovieLens is a movie recommendation service that has **9,742 movies** and **100,836 ratings** from **610 users**. The dataset was updated on September 26, 2018. There are three datasets but will only be needing two of them, the ratings and the movies. \n",
    "\n",
    "The movie dataset has the **movie IDs, titles, and genres** for the movie. This was used mainly for conversions but could potentially be used for the genres as it was explored for a bit. \n",
    "Ratings dataset have the most pieces of information as it has ratings and timestamps. The ratings were scaled from **0 to 5 with a .5 step**. \n",
    "\n",
    "Exploring the datasets have some interesting facts concerning the dataset. For one, the distribution is slightly skewed left with the **mean** rating happening to be roughly **3.5**. There are a handful of users that contributes to the reviews given which may have bias depending on who the user was. **Most movies were not rated below 3** so it might be difficult to determine a good movie to recommend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the dataset and load them in with proper names ###\n",
    "### Links were not used for this project ###\n",
    "\n",
    "import pandas as pd\n",
    "import helper as hp\n",
    "movies = pd.read_csv('Data/movies.csv')\n",
    "ratings = pd.read_csv('Data/ratings.csv')\n",
    "tags = pd.read_csv('Data/tags.csv')\n",
    "\n",
    "# links = pd.read_csv('ml-latest-small/links.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration Information\n",
    "* There are **9,742** movies in the dataset\n",
    "* There are **100,836** ratings and 610 users \n",
    "* There are **58** users that make up the **3,683** tags added\n",
    "* Genres are separated by | if there are more than one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9742 entries, 0 to 9741\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   movieId  9742 non-null   int64 \n",
      " 1   title    9742 non-null   object\n",
      " 2   genres   9742 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 228.5+ KB\n"
     ]
    }
   ],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperations\n",
    "Timestamp was not needed for this project and so it was removed from the rating data frame. <br>\n",
    "Genre will be used to compare so I made all the letters lowercase to standardize the words when comparing. <br>\n",
    "Lastly, a training and test set was set up to test the accuracy of the model when performing the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove the timestamp column ### \n",
    "ratings.drop('timestamp', axis = 1, inplace = True)\n",
    "movies['genres'] = movies['genres'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "### Reader to use surprise libraries and create training set and testset ### \n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings,reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.3, random_state= 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary libraries for modeling and validating and testing accuracy of the models ### \n",
    "from surprise.prediction_algorithms import knns\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "We used **surprise library** to create a recommendation system using different algorithms. The surprise library has bulting models and testing function that can be easily used in this project. The Memory-Based models are **KnnBasic**, **KnnBaseline**, and **KnnWithMeans**. The Model-based modeling algorithm is **SVD or Singular Value decomposition**. After finding the best base model, a grid search is used to find the best parameters. We also tried to use different similarity conditions to see if that also improves the model. \n",
    "\n",
    "The metric used for evaluation is **RSME or the Root Square Mean Error**. This metric gives the average amount that each predicted rating was off by. Ideally, we would want a score close to 0 to predict similarly to other rated movies in the test set. \n",
    "\n",
    "The **default paramenters** are used to compare the model. We perform a grid search once the best performing model is choosing. We will be using **cross validation** function to get an average on the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Memory Based Methods \n",
    "**Three different** variation of KNN with all the cosine similarity to compare and all user based for these models. <br>\n",
    "Cosine similarity perform better for KnnBasic and KnnWithMeans, but the best performing model, **KnnBaseline** perform best with **Pearson similarity** instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnnBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.9799136359416775\n",
      "RMSE: 0.9819\n",
      "Testset RSME Score:  0.9818952607403942\n"
     ]
    }
   ],
   "source": [
    "sim_cosine = {\"name\": \"cosine\", \"user_based\": True}\n",
    "basic = knns.KNNBasic(sim_options=sim_cosine, random_state = 69)\n",
    "cv_basic = cross_validate(basic, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "basic.fit(trainset)\n",
    "basic_pred = basic.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_basic)\n",
    "print('Testset RSME Score: ',accuracy.rmse(basic_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnnWithMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.9081131144335789\n",
      "RMSE: 0.9103\n",
      "Testset RSME Score:  0.910278017316939\n"
     ]
    }
   ],
   "source": [
    "knn_means = knns.KNNWithMeans(sim_options=sim_cosine, random_state = 69)\n",
    "cv_means = cross_validate(knn_means, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_means.fit(trainset)\n",
    "predictions = knn_means.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_means)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.886005608915299\n",
      "RMSE: 0.8883\n",
      "Testset RSME Score:  0.8882986936389531\n"
     ]
    }
   ],
   "source": [
    "knn_baseline = knns.KNNBaseline(sim_options=sim_cosine, random_state = 69)\n",
    "cv_baseline = cross_validate(knn_baseline, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_baseline)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVD algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validate RMSE Score:  0.8734758391251052\n",
      "RMSE: 0.8833\n",
      "Testset RSME Score:  0.883294923683403\n"
     ]
    }
   ],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "svd = SVD(random_state = 69)\n",
    "svd_cv = cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)['test_rmse'].mean()\n",
    "svd.fit(trainset)\n",
    "svd_pred = svd.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', svd_cv)\n",
    "print('Testset RSME Score: ',accuracy.rmse(svd_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KnnBaseline vs SVD\n",
    "Both KnnBaseline and SVD performed the best when modeling so we need to determine which will be used for the final model. A gridsearch is used to see if there is a better combination of parameters to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "# params = {'k':[10, 20, 30, 40, 50],\n",
    "#           'min_k': [1, 2, 3, 4,5,6,7,8,9,10],\n",
    "#           'random_state':[69]\n",
    "#          }\n",
    "# g_s_baseline = GridSearchCV(knns.KNNBaseline,param_grid=params,n_jobs=-1)\n",
    "# g_s_baseline.fit(data)\n",
    "# g_s_baseline.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.8732087980225783\n",
      "RMSE: 0.8753\n",
      "Testset RSME Score:  0.8753167080242304\n"
     ]
    }
   ],
   "source": [
    "sim_pearson = {\"name\": \"pearson\", \"user_based\": True}\n",
    "knn_baseline = knns.KNNBaseline(k = 30, min_k = 6, sim_options=sim_pearson, random_state = 69)\n",
    "cv_baseline_best = cross_validate(knn_baseline, data, measures=['RMSE'], cv=3, verbose=False)['test_rmse'].mean()\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "print('Average Cross Validate RMSE Score: ', cv_baseline_best)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "# params = {'n_factors': [20, 50, 100],\n",
    "#          'reg_all': [0.02, 0.05, 0.1],\n",
    "#           'lr_all': [.001, .002, .003, .004, .005],\n",
    "#          'random_state':[69]\n",
    "#          }\n",
    "# g_s_svd = GridSearchCV(SVD,param_grid=params,n_jobs=-1,cv = 5)\n",
    "# g_s_svd.fit(data)\n",
    "# g_s_svd.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validate RMSE Score:  0.8694868695043445\n",
      "RMSE: 0.8780\n",
      "Testset RSME Score:  0.8779828055268748\n"
     ]
    }
   ],
   "source": [
    "best_svd = SVD(n_factors= 50, reg_all = 0.05, lr_all= 0.005, random_state = 69)\n",
    "cv_svd = cross_validate(best_svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)['test_rmse'].mean()\n",
    "best_svd.fit(trainset)\n",
    "svd_pred = best_svd.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_svd)\n",
    "print('Testset RSME Score: ',accuracy.rmse(svd_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "KnnBaseline perform **slightly better** than the SVD algorithm so we will using KnnBaseline as our main model. The average validations shows that SVD performs slightly better as well. For the given random state, I will be using KnnBaseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item to Item Comparisons\n",
    "There happen to be more movies than a user, so it would be ideal to see if item-based filtering would perform better than the user-based model. Unfortunately, it **did not perform better** than the user based similarity model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train another model but item to item comparison instead ###\n",
    "# sim_pearson = {\"name\": \"pearson\", \"user_based\": False}\n",
    "# clf = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "# params = {'k':[10, 20, 30, 40, 50],\n",
    "#           'min_k': [1, 2, 3, 4,5,6,7,8,9,10],\n",
    "#           'random_state':[69]\n",
    "#          }\n",
    "# g_s_baseline_item = GridSearchCV(knns.KNNBaseline,param_grid=params,n_jobs=-1)\n",
    "# g_s_baseline_item.fit(data)\n",
    "# g_s_baseline_item.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Average Cross Validate RMSE Score:  0.8650697606603452\n",
      "RMSE: 0.8753\n",
      "Testset RSME Score:  0.8753181775899509\n"
     ]
    }
   ],
   "source": [
    "knn_baseline_item = knns.KNNBaseline(k = 30, min_k = 7,sim_options=sim_pearson, random_state = 69)\n",
    "cv_svd = cross_validate(knn_baseline_item, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)['test_rmse'].mean()\n",
    "knn_baseline_item.fit(trainset)\n",
    "predictions = knn_baseline_item.test(testset)\n",
    "\n",
    "print('Average Cross Validate RMSE Score: ', cv_svd)\n",
    "print('Testset RSME Score: ',accuracy.rmse(predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=275, iid=4306, r_ui=3.0, est=4.216200786371028, details={'actual_k': 30, 'was_impossible': False})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Prediction(uid=275, iid=4306, r_ui=3.0, est=4.216200786371028, details={'actual_k': 30, 'was_impossible': False})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### First look at the predictions ### \n",
    "import numpy as np\n",
    "predictions_user = knn_baseline.test(testset)\n",
    "predictions_item = knn_baseline_item.test(testset)\n",
    "\n",
    "display(predictions_item[0], predictions_user[0])\n",
    "\n",
    "actual = np.array([real[2] for real in predictions_user])\n",
    "pred_1 = np.array([pred[3] for pred in predictions_user])\n",
    "pred_2 = np.array([pred[3] for pred in predictions_item])\n",
    "\n",
    "pred_combined = (pred_1 + pred_2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8747062525807481"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(actual, pred_combined, squared= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Model Results \n",
    "Comparing all the models, there is a **slight improvement** in the RSME score with the combined ratings of the item and user models. <br>\n",
    "This could be interpreted that a combination of filtering can lead to a better score which in turn gives more **accurate** ratings. \n",
    "\n",
    "|            | **Knn_Baseline_user** | **Knn_Baseline_item** | **Combined Rating** |\n",
    "|------------|:---------------------:|:---------------------:|:-------------------:|\n",
    "| RSME Score | .8776                 | .8887                | **.8717**           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Based:0.9884395627200159\n",
      "Item Based:0.988463546482642\n",
      "Combined Based:0.9884715381213082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "print(\"User Based:{}\".format(ndcg_score(np.asarray([actual]),np.asarray([pred_1]))))\n",
    "print(\"Item Based:{}\".format(ndcg_score(np.asarray([actual]),np.asarray([pred_2]))))\n",
    "print(\"Combined Based:{}\".format(ndcg_score(np.asarray([actual]),np.asarray([pred_combined]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Example 1 Recommendation\n",
    "Examples of the model work to provide recommendations based on the user given. **User 1** was used for showing how the system would work.  <br>\n",
    "Both of the graphs are from the model's prediction with the watched movies removed. <br>\n",
    "Id is still used for readability. \n",
    "\n",
    "This would give a feel on how the system would work for User 1 and the recommendations that it would give. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Graph comparison\n",
    "\n",
    "From the predicted ratings for user 1, we can see **a slight difference** in user similarity vs item similarity between movies. Interestingly, the item comparison has all the movies rated above three which goes inside with user 1 trend on rating movies. User 1 doesn't seem so **rate movies lower than three** often and therefore most movies would be rated similarly. Due to the number of users, there is a bigger disparity between users with the movies they watched naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_user = hp.user_top_5(user_id = 1, model = knn_baseline, review_df = ratings, movies = movies)        ## The list of movies with the best model\n",
    "item_item = hp.user_top_5(user_id = 1, model = knn_baseline_item, review_df = ratings, movies = movies)   ## The list of movies with the best model based off items\n",
    "x_1 = [ID[0] for ID in user_user]   ## All movie id numbers predicted\n",
    "x_2 = [ID[0] for ID in item_item]   ## All movie id numbers predicted from item based \n",
    "y_1 = [ID[1] for ID in user_user]   ## Movie Ratings from user based model\n",
    "y_2 = [ID[1] for ID in item_item]   ## Movie Ratings from item based model\n",
    "\n",
    "fig, ax = plt.subplots(ncols= 2, figsize = (10,5))\n",
    "\n",
    "ax[0].set_title('Users Based Predictions')\n",
    "ax[0].set_xlabel('Movie IDs')\n",
    "ax[0].set_ylabel('Ratings (0-5)')\n",
    "ax[0].set_ylim(0, 5.1)\n",
    "ax[0].scatter(x_1, y_1, label = 'User - User', color = 'green');\n",
    "ax[0].axhline(np.mean(y_2), color = 'red',label= round(np.mean(y_1), 2))\n",
    "ax[0].legend();\n",
    "\n",
    "ax[1].set_title('Items Based Predictions')\n",
    "ax[1].set_ylim(0, 5.1)\n",
    "ax[1].set_ylabel('Ratings (0-5)')\n",
    "ax[1].set_xlabel('Movie IDs')\n",
    "ax[1].scatter(x_2, y_2, label = 'Item - Item');\n",
    "ax[1].axhline(np.mean(y_2), color = 'red', label= round(np.mean(y_2), 2))\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Displays the distribution of ratings for the ratings dataset. \n",
    "ratings['rating'].loc[ratings['userId'] == 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print out the results from the models ###\n",
    "\n",
    "print('Movies from User Filter')\n",
    "for item in user_user[0:5]:\n",
    "    print(movies['title'].loc[movies['movieId'] == item[0]].values)\n",
    "print('Movies from Movie Filter')\n",
    "for item in item_item[0:5]:\n",
    "    print(movies['title'].loc[movies['movieId'] == item[0]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [id[0] for id in sorted(user_user, key= lambda x:x[0])]    ## Take only the ID number from the list of tuples\n",
    "\n",
    "rate_1 = np.array([user[1] for user in sorted(user_user, key= lambda x:x[0])])   ## Make numpy array for the first list\n",
    "rate_2 = np.array([item[1] for item in sorted(item_item, key= lambda x:x[0])])   ## Make numpy array for second list\n",
    "\n",
    "rate_comb = (rate_1 + rate_2)/2   ## Take the average of the two \n",
    "                   \n",
    "rated_df = pd.DataFrame(zip(ids, rate_comb), columns= ['Movie_ID', 'Combined_Rating'])   ## Create a new dataframe with the new values     \n",
    "hp.id_title(rated_df.sort_values('Combined_Rating', ascending= False)[0:5]['Movie_ID'].values, movies).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_1_top_genres = hp.get_genres(ratings.loc[ratings['userId']==1].sort_values('rating', ascending = False)[0:5]['movieId'].values, movies)\n",
    "pred_user_1 = hp.user_top_5(knn_baseline, ratings, 1, movies)\n",
    "user_suggestions = hp.get_genres([ID[0] for ID in pred_user_1], movies)\n",
    "movie_index = []\n",
    "for index, suggestion in enumerate(user_suggestions):\n",
    "    if suggestion in user_1_top_genres:\n",
    "        movie_index.append(index - 1)\n",
    "    if len(movie_index) == 5:\n",
    "        break\n",
    "top_movie_genre = [pred_user_1[value][0] for value in movie_index]\n",
    "hp.id_title(top_movie_genre, movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Results \n",
    "The **Top movies** for these methods are as followed for user 1:\n",
    "\n",
    "| Weighted Ranks Sum | Average Ratings Movies | Includes Favorite Genres |\n",
    "|:---:|:---:|:---:|\n",
    "| **'Shawshank Redemption, The (1994)'** | **'Shawshank Redemption, The (1994)'** | **Ghost in the Shell (Kôkaku kidôtai) (1995)** |\n",
    "| **'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)'** | **'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)'** | Singin' in the Rain (1952) |\n",
    "| **'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)'** | **'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)'** | Notorious (1946) |\n",
    "| **'Ghost in the Shell (Kôkaku kidôtai) (1995)'** | 'Lord of the Rings: The Fellowship of the Ring, The (2001)' | Sicario (2015) |\n",
    "| 'Wallace & Gromit: The Best of Aardman Animation (1996)' | 'High Noon (1952)' | Big Short, The (2015) |\n",
    "\n",
    "Some similar movies were suggested but there is no way to tell whether a user would like the recommendation that was created. <br> \n",
    "**The weighted ranked** recommendations consider all the movies and their respective ranking to give a **more diverse spread**. <br> \n",
    "**The average ratings** make sure that **only highly rated movies** are shown as a recommendation from the prediction ratings. <br> \n",
    "Using their **favorite genre** can filter out movies they wouldn't normally watch with ones that they are **familiar** with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cold Starts\n",
    "There are **2 cold starts** scenarios when dealing with recommendation systems. <br>\n",
    "If a **new user** is added to the system, how would **the system suggest recommendations**? There would be **no prior ratings** from the users and so there is no way to **compare them to the others** without more information. <br> \n",
    "If a **new movie** is added to the system, how would the system **suggest the movie to users**? The new movie won't be suggested since none of the other users have seen the movie as well. This would include all the movies in the movies list that didn't have a user rate the movie. One user a minimum needs to rate it in order for the model to recommend it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Case\n",
    "For **new users,** we can recommend them the **highest-rated movie** or the most-rated movie to be recommended. We can also have the user give more information such as a know movie they watched and that rating or a given genre that the user likes. For this project, I can recommend the **most-rated movie** or the **most-reviewed movie.** Each has there advantage and disadvantages and have there uses if labeled properly on websites. \n",
    "The higher-rated movies can give users movies that will be liked but don't consider the number of users to that movie. <br> \n",
    "The **most rated movies** consider the fact that users have actively watched and rated them. This method is also **more likely** to change over time when more people rate the movies which can make for **better recommendations**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recommend the most rated movies without any distinguish ## \n",
    "hp.user_top_5(movies= movies, review_df= ratings)\n",
    "## OR ## \n",
    "\n",
    "## Recommend the most reviewed movies from all of the avaiable users currently ## \n",
    "movies[movies['movieId'].isin(ratings[['movieId', 'userId']].groupby('movieId').count().sort_values('userId', ascending = False).index[0:5])]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Most Rated Movies | Most Popular Movies |\n",
    "|---|---|\n",
    "| Paper Birds (Pájaros de papel) (2010) | Pulp Fiction (1994) |\n",
    "| Act of Killing, The (2012) | Shawshank Redemption, The (1994) |\n",
    "| Jump In! (2007) | Forrest Gump (1994) |\n",
    "| Human (2015) | Silence of the Lambs, The (1991) |\n",
    "| L.A. Slasher (2015) | Matrix, The (1999) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new movie, we need to implement the movie in the system and be able to recommend it to users. \n",
    "One potential solution was to give a rating to the movie, the average of that movie's genre. Although it gave a ranking to the movie, they would most likely won't get recommended since for this dataset at least, has to be at least rated 3 or above. \n",
    "\n",
    "We will use the movie dataset to help decide the rating for the movie and if it should be rated. To compare other movies, we used their genres to obtain an average rating.\n",
    "Similarly to what other applications does are allow users to see recommendations for new items for users to look into along with the top choices as well. Keeping this method in mind, we can narrow it down further by letting the user's past movie genres limit the new movies to see whether or not to recommend them to the user or not. \n",
    "We can see that user 1's top-rated movies mainly have the genre, and action in them. That would mean that ideally, we would give more weight to a new action movie than any other movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.new_movie('action', movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[movies['movieId'].isin(ratings.loc[ratings['userId'] == 1]['movieId'].values)]['genres'].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Two models were created to help recommend the **best types of movies to a given user**. <br> \n",
    "The **first** model is based on the **similarities of users** and the movies they like. The idea would be that users have similar likes and dislike and should be recommended to them. They should rate the movies similarly. <br> \n",
    "The **second** model compares the ratings from movies and **compares them to similar-rated movies**. If the movies have similar ratings from one another, a user may rate that movie similarly as well. Both of the models didn't perform well with a **.8776 and .8899 RSME score**. <br> RSME score gives an average rating that the predicted values gave from the true value. That would mean that a movie was rated on average higher by **.87 to .89** more than the true values. In the attempt to see if **combining the models together**, there was a slight increase in predictions but not much. \n",
    "\n",
    "In order to better the model, some measures were placed when dealing with cold starts. **Cold starts** are situations in a given system that need some help to get started before being implemented correctly. \n",
    "The **first** cold start to consider is a new user. Since there are **no prior ratings** from the user, there is no way to recommend movies with the current system. Two different solutions are to recommend the **most popular movies** or attempt to grab more information from the user to get some sort of preference for a movie. \n",
    "\n",
    "The **second** cold start to consider is a new movie. **A new movie** will not have any users that have watched and rated the movie yet. Therefore, the system would **never recommend** the movie with the current system. To help remedy this, we implemented **content-based filtering** that helps pick movies if the user has a similar interest in the genre given to the new movie. Since we have **a history of movies and their genres**, we can still compare them for the users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "This model isn't without its limitations as several aspects make the model less effective.\n",
    "\n",
    "1. The distrubition is **skewed to the left** with many of the ratings are higher than 3 with **few rated between 0 - 2.5**. This can be contributed to the fact that many people won't leave reviews unprompted.\n",
    "2. The model doesn't have anything to combat review bombing which has happened to many movies recently to **negatively impact the movie's performance**. This can be difficult since it's hard to distinguish between actually truthful ratings or not. \n",
    "3. There were **many ratings from the same users**. There can be a **bias** between users especially when the majority of the ratings are from them. These users could also be critics that may be very helpful to the average moviegoer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "There are also some ways to improve the model that can be added to the project to further the accuracy for users.\n",
    "\n",
    "1. The movie dataset had genres as a column. A previous project shows that **certain combinations perform better at the box office**. If we can incorporate this in the model and know what the users' favorite genres are. \n",
    "2. **More data will improve the model**. There was a bigger dataset that was also created and could be used, with enough time, for a better model. \n",
    "3. The data should include more ratings with a **wider range of ratings** from the users. Ideally should be a bell curve depending on if an equal number of bad and good movies are included in the movies list. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
